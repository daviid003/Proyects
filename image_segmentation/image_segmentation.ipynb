{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U git+https://github.com/albumentations-team/albumentations\n",
    "# %pip install --upgrade opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/parth1620/Human-Segmentation-Dataset-master.git\n",
    "# import sys\n",
    "# sys.path.append('/content/Human-Segmentation-Dataset-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class helper:\n",
    "    def show_image(image,mask,pred_image = None):\n",
    "        if pred_image == None:\n",
    "            \n",
    "            f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "            \n",
    "            ax1.set_title('IMAGE')\n",
    "            ax1.imshow(image.permute(1,2,0).squeeze(),cmap = 'gray')\n",
    "            \n",
    "            ax2.set_title('GROUND TRUTH')\n",
    "            ax2.imshow(mask.permute(1,2,0).squeeze(),cmap = 'gray')\n",
    "            \n",
    "        elif pred_image != None :\n",
    "            \n",
    "            f, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(10,5))\n",
    "            \n",
    "            ax1.set_title('IMAGE')\n",
    "            ax1.imshow(image.permute(1,2,0).squeeze(),cmap = 'gray')\n",
    "            \n",
    "            ax2.set_title('GROUND TRUTH')\n",
    "            ax2.imshow(mask.permute(1,2,0).squeeze(),cmap = 'gray')\n",
    "            \n",
    "            ax3.set_title('MODEL OUTPUT')\n",
    "            ax3.imshow(pred_image.permute(1,2,0).squeeze(),cmap = 'gray')\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss, FocalLoss, TverskyLoss, JaccardLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE = 'C:\\\\Users\\\\davii\\\\OneDrive\\\\Escritorio\\\\Cursos\\\\image_segmentation\\\\Human-Segmentation-Dataset-master\\\\train.csv'\n",
    "DATA_DIR = 'C:\\\\Users\\\\davii\\\\OneDrive\\\\Escritorio\\\\Cursos\\\\image_segmentation\\\\Human-Segmentation-Dataset-master'\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "EPOCH = 25\n",
    "LR = 0.003\n",
    "IMAGE_SIZE = 320\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "ENCODER = 'timm-efficientnet-b0'\n",
    "WEIGHTS = 'imagenet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[1]\n",
    "\n",
    "image_path = row.images\n",
    "mask_path = row.masks\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax1.set_title('IMAGE')\n",
    "ax1.imshow(image);\n",
    "\n",
    "ax2.set_title('MASK')\n",
    "ax2.imshow(mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size =.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentations():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.HorizontalFlip(p=.5),\n",
    "        A.VerticalFlip(p=.5)\n",
    "    ])\n",
    "def get_valid_augmentations():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, augmentations):\n",
    "        self.df = df\n",
    "        self.augmentations = augmentations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        image_path = row.images\n",
    "        mask_path = row.masks\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # shape (h, w, c)\n",
    "        mask = np.expand_dims(mask, axis = -1)\n",
    "\n",
    "        if self.augmentations:\n",
    "            data = self.augmentations(image = image, mask = mask)\n",
    "            image = data['image']\n",
    "            mask = data['mask']\n",
    "        \n",
    "        #(h, w, c) -> (c, h, w)\n",
    "        image = np.transpose(image, (2,0,1)).astype(np.float32) # put c on the first position\n",
    "        mask = np.transpose(mask, (2,0,1)).astype(np.float32) # put c on the first position\n",
    "\n",
    "        image = torch.Tensor(image) / 255.\n",
    "        mask = torch.round(torch.Tensor(mask) / 255.) # round mask 0 to 1\n",
    "    \n",
    "        return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SegmentationDataset(train_df, get_train_augmentations())\n",
    "validset = SegmentationDataset(valid_df, get_valid_augmentations())\n",
    "\n",
    "print(\"Size of Trainset: \", len(trainset))\n",
    "print(\"Size of Validset: \", len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 75\n",
    "image, mask = trainset[index]\n",
    "helper.show_image(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Batches of Trainloader: \",len(trainloader))\n",
    "print(\"Batches of Validloader: \",len(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in trainloader: \n",
    "    break\n",
    "print(\"One batch image shape: \", image.shape)\n",
    "print(\"One batch mask shape: \", mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.arc = smp.Unet(\n",
    "            encoder_name = ENCODER,\n",
    "            encoder_weights = WEIGHTS,\n",
    "            in_channels = 3, \n",
    "            classes = 1,\n",
    "            activation = None\n",
    "        )\n",
    "    def forward(self, images, masks = None):\n",
    "        logits = self.arc(images)\n",
    "        if masks != None:  # If mask doesn't exists, we dont have loss\n",
    "            loss1 = JaccardLoss(mode='binary')(logits, masks)\n",
    "            # loss2 = JaccardLoss(logits,masks)\n",
    "            return logits, loss1 #+ loss2\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentationModel()\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(data_loader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "\n",
    "    for images, masks in tqdm(data_loader): # tqdm is for progress bar\n",
    "        # Load data in GPU\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(images, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(data_loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad(): # put the grad to false (not use in eval)\n",
    "        for images, masks in tqdm(data_loader):\n",
    "            # Load data in GPU\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            logits, loss = model(images, mask)\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.Inf # infinity\n",
    "\n",
    "for i in range (EPOCH):\n",
    "    train_loss = train_function(trainloader, model, optimizer)\n",
    "    valid_loss = eval_function(validloader, model)\n",
    "    if validloader < best_valid_loss:\n",
    "        torch.save(model.state_dict(),'best_model_pytorch')\n",
    "        print('Saved model ', str(i+1))\n",
    "        best_valid_loss = valid_loss\n",
    "    print(\"Epoch: \",str(i+1), \" Train_loss: \", train_loss, \" Valid_loss: \", valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 20\n",
    "# model.load_state_dict(torch.load(route_best_model.pt))\n",
    "image, mask = validset[index]\n",
    "logits_mask = model(image.to(DEVICE).unsqueeze(0)) # (channel, higth, weight) -> (1,C,H,W) \n",
    "pred_mask = torch.sigmoid(logits_mask)\n",
    "pred_mask = (pred_mask > .5)* 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_image(image, pred_mask.detach().cpu().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "273501e4657df640dbdad69edccc8be2efbba911843ba2e0a0e415caaf29ad52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
